# 02 - Architecture: System Design & Decisions

---

## üèóÔ∏è "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Architecture ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏ô‡πà‡∏≠‡∏¢"

### High-Level Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CLIENT LAYER                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ           SvelteKit Frontend (localhost:5173)                ‚îÇ‚îÇ
‚îÇ  ‚îÇ      Svelte 5 Runes + TailwindCSS + shadcn-svelte           ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ REST API + SSE Streaming
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     API LAYER                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ           FastAPI Backend (localhost:8000)                   ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Routes  ‚îÇ ‚îÇMiddleware‚îÇ ‚îÇ   Auth   ‚îÇ ‚îÇRate Limit‚îÇ       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                                                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ              SERVICE LAYER                        ‚îÇ       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  RAG   ‚îÇ ‚îÇ Agent  ‚îÇ ‚îÇWorkflow‚îÇ ‚îÇBilling ‚îÇ     ‚îÇ       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇService ‚îÇ ‚îÇ Engine ‚îÇ ‚îÇ Engine ‚îÇ ‚îÇService ‚îÇ     ‚îÇ       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ       ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     DATA LAYER                                   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ  ‚îÇ   LiteLLM    ‚îÇ  ‚îÇ    Redis     ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  + pgvector  ‚îÇ  ‚îÇ    Proxy     ‚îÇ  ‚îÇ              ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Users      ‚îÇ  ‚îÇ ‚Ä¢ Gemini     ‚îÇ  ‚îÇ ‚Ä¢ Rate Limit ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Documents  ‚îÇ  ‚îÇ ‚Ä¢ OpenAI     ‚îÇ  ‚îÇ ‚Ä¢ Cache      ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Chunks     ‚îÇ  ‚îÇ ‚Ä¢ Anthropic  ‚îÇ  ‚îÇ ‚Ä¢ Sessions   ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Embeddings ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ  ‚îÇ    Stripe    ‚îÇ  ‚îÇ    MinIO     ‚îÇ                             ‚îÇ
‚îÇ  ‚îÇ   (Billing)  ‚îÇ  ‚îÇ  (Storage)   ‚îÇ                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üßÖ Layer Pattern: ‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏¢‡∏Å Route / Service / Model?

### The Problem with Fat Controllers

‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô codebase ‡∏ó‡∏µ‡πà route handler ‡∏¢‡∏≤‡∏ß 200 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î ‡∏ó‡∏≥‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô function ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß:

```python
# ‚ùå Anti-pattern: Fat Controller
@router.post("/documents")
async def upload_document(file: UploadFile, db: AsyncSession):
    # Validate file
    if file.content_type not in ALLOWED_TYPES:
        raise HTTPException(400, "Invalid file type")

    # Extract text (50 lines)
    if file.content_type == "application/pdf":
        # ... PDF extraction logic
    elif file.content_type == "application/docx":
        # ... DOCX extraction logic

    # Chunk text (30 lines)
    chunks = []
    for i in range(0, len(text), CHUNK_SIZE):
        # ... chunking logic

    # Embed chunks (20 lines)
    embeddings = []
    for chunk in chunks:
        # ... embedding logic

    # Save to database (30 lines)
    document = Document(...)
    db.add(document)
    # ... more DB operations

    return document
```

**Problems:**
- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ reuse logic ‡πÑ‡∏î‡πâ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ process document ‡∏à‡∏≤‡∏Å CLI?)
- Test ‡∏¢‡∏≤‡∏Å‡∏°‡∏≤‡∏Å (‡∏ï‡πâ‡∏≠‡∏á mock HTTP layer)
- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô PDF library ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÉ‡∏ô route handler

### The Solution: Layered Architecture

```python
# ‚úÖ Clean Architecture

# Layer 1: Route (HTTP concerns only)
@router.post("/documents")
async def upload_document(
    file: UploadFile,
    db: AsyncSession = Depends(get_db),
    user: User = Depends(get_current_user)
):
    document = await document_service.process_and_store(
        file=file,
        user_id=user.id,
        db=db
    )
    return BaseResponse(trace_id=get_trace_id(), data=document)


# Layer 2: Service (Business Logic)
class DocumentService:
    async def process_and_store(
        self,
        file: UploadFile,
        user_id: UUID,
        db: AsyncSession
    ) -> Document:
        # Orchestrate the workflow
        text = await self.document_processor.extract(file)
        chunks = self.text_chunker.chunk(text)
        embeddings = await self.embedding_service.embed_batch(chunks)
        return await self.repository.save(user_id, chunks, embeddings, db)


# Layer 3: Specialized Services
class DocumentProcessor:
    async def extract(self, file: UploadFile) -> str:
        """Extract text from any supported format"""

class TextChunker:
    def chunk(self, text: str) -> list[Chunk]:
        """Split text into overlapping chunks"""

class EmbeddingService:
    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings via LiteLLM"""
```

### Benefits Realized

| Aspect | Before | After |
|--------|--------|-------|
| **Testing** | Mock HTTP, DB, LLM ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î | Test service ‡πÅ‡∏¢‡∏Å, mock dependencies |
| **Reusability** | ‡∏ó‡∏≥ CLI ‡∏ï‡πâ‡∏≠‡∏á copy code | ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å service ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô |
| **Changes** | ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô PDF library ‡πÅ‡∏Å‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏µ‡πà | ‡πÅ‡∏Å‡πâ‡πÅ‡∏Ñ‡πà DocumentProcessor |
| **Readability** | 200 line function | 20 line function + clear services |

---

## üîå Dependency Injection: ‡∏ó‡∏≥‡πÑ‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç?

### FastAPI Dependencies ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ

```python
# 1. Database Session - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å request, cleanup ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    async with async_session() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise

# 2. Current User - verify JWT, load user
async def get_current_user(
    token: str = Depends(oauth2_scheme),
    db: AsyncSession = Depends(get_db)
) -> User:
    payload = verify_jwt(token)
    user = await get_user_by_id(db, payload["sub"])
    if not user or not user.is_active:
        raise HTTPException(401, "Invalid user")
    return user

# 3. Request Context - trace_id ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö distributed tracing
def get_context() -> RequestContext:
    ctx = _request_context.get()
    if ctx is None:
        raise RuntimeError("No request context")
    return ctx

# Usage in route
@router.get("/me")
async def get_me(
    user: User = Depends(get_current_user),  # Auto-injected
    db: AsyncSession = Depends(get_db)       # Auto-injected
):
    return user
```

### Why DI Instead of Global State?

```python
# ‚ùå Global state - hard to test, not thread-safe
db_session = create_session()

@router.get("/users")
async def get_users():
    return await db_session.query(User).all()  # Which session? Race condition?


# ‚úÖ Dependency Injection - explicit, testable
@router.get("/users")
async def get_users(db: AsyncSession = Depends(get_db)):
    return await db.query(User).all()  # Clear ownership

# In tests
async def test_get_users():
    async with test_session() as db:
        result = await get_users(db=db)  # Inject test DB
```

---

## üîÄ "‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Async ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÅ‡∏£‡∏Å?"

### The Story

‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Ñ ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ sync ‡∏´‡∏£‡∏∑‡∏≠ async

**Sync Option:**
- Simple, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
- Flask, Django (traditional)
- ‡πÅ‡∏ï‡πà block thread ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏≠ I/O

**Async Option:**
- ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤
- FastAPI, asyncio
- ‡πÑ‡∏°‡πà block, handle concurrent requests ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤

### Analysis

‡∏î‡∏π I/O operations ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö:

```
1. Database queries     ‚Üí 50-200ms
2. LLM API calls       ‚Üí 1-10 seconds
3. Embedding API calls ‚Üí 100-500ms
4. File storage        ‚Üí 50-100ms
```

**LLM calls ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å!** ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ sync ‡πÅ‡∏•‡∏∞‡∏°‡∏µ 100 concurrent users:

```
Sync: 100 requests √ó 5 seconds = 100 threads blocked
Async: 100 requests √ó 5 seconds = 1 thread, 100 coroutines
```

### Decision: Async Everywhere

```python
# All external calls are async
async def retrieve_and_answer(query: str, db: AsyncSession):
    # 1. Embed query (100ms, non-blocking)
    embedding = await embedding_service.embed_query(query)

    # 2. Vector search (50ms, non-blocking)
    chunks = await vector_store.search(db, embedding)

    # 3. LLM call (5000ms, non-blocking!)
    response = await llm_client.complete(build_prompt(query, chunks))

    return response
```

**Result:** Single process handles hundreds of concurrent LLM requests

### Trade-offs Encountered

| Challenge | Solution |
|-----------|----------|
| Async SQLAlchemy learning curve | ‡∏≠‡πà‡∏≤‡∏ô docs, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à session lifecycle |
| Connection pool exhaustion | Proper pool size tuning |
| Debugging async stack traces | Better logging, trace IDs |
| Testing async code | pytest-asyncio, async fixtures |

---

## üóÑÔ∏è "‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ PostgreSQL + pgvector?"

### The Alternatives Considered

| Option | Pros | Cons |
|--------|------|------|
| **Pinecone** | Managed, fast, scalable | $$, separate service, sync complexity |
| **Weaviate** | Open source, feature-rich | Another DB to manage, operational overhead |
| **Chroma** | Simple, embedded | Not production-ready, SQLite-based |
| **pgvector** | Integrated with Postgres | Newer, less optimized for billions |

### Why pgvector Won

**1. Single Database = Simpler Operations**

```python
# Without pgvector (2 databases)
async def save_document(doc, chunks, embeddings):
    # Save to Postgres
    await postgres.insert(doc)
    await postgres.insert_many(chunks)

    # Sync to Pinecone (what if this fails?)
    await pinecone.upsert(embeddings)

    # Consistency nightmare!


# With pgvector (1 database)
async def save_document(doc, chunks, embeddings):
    async with db.begin():
        db.add(Document(**doc))
        for chunk, emb in zip(chunks, embeddings):
            db.add(DocumentChunk(content=chunk, embedding=emb))
        # All or nothing - ACID transaction!
```

**2. JOINs ‡∏Å‡∏±‡∏ö Relational Data**

```sql
-- pgvector allows filtering BEFORE vector search
SELECT c.content, c.embedding <=> $query AS distance
FROM document_chunks c
JOIN documents d ON c.document_id = d.id
WHERE d.user_id = $user_id              -- Filter first
  AND d.project_id = $project_id        -- Reduce search space
ORDER BY distance
LIMIT 10;
```

**3. Cost & Simplicity**

- ‡πÑ‡∏°‡πà‡∏°‡∏µ additional monthly cost (vs Pinecone ~$70/month minimum)
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á manage separate service
- Standard Postgres backup/restore

### Limitations Accepted

- **Scale:** pgvector handles millions, not billions (acceptable for our use case)
- **Speed:** HNSW index ‡∏ï‡πâ‡∏≠‡∏á tune ‡πÄ‡∏≠‡∏á (vs managed services auto-optimize)

---

## üîÑ "Request Flow ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"

### Complete Request Lifecycle

```
1. Request Arrives
   ‚îÇ
   ‚ñº
2. CORS Middleware ‚Üí Check allowed origins
   ‚îÇ
   ‚ñº
3. TraceContextMiddleware ‚Üí Generate trace_id, attach to request
   ‚îÇ
   ‚ñº
4. MetricsMiddleware ‚Üí Start timing
   ‚îÇ
   ‚ñº
5. Route Handler
   ‚îÇ
   ‚îú‚îÄ‚îÄ Dependency: get_db() ‚Üí Create DB session
   ‚îú‚îÄ‚îÄ Dependency: get_current_user() ‚Üí Verify JWT
   ‚îú‚îÄ‚îÄ Dependency: RateLimiter() ‚Üí Check rate limit
   ‚îÇ
   ‚ñº
6. Service Layer ‚Üí Business logic
   ‚îÇ
   ‚ñº
7. Response
   ‚îÇ
   ‚îú‚îÄ‚îÄ Commit DB transaction
   ‚îú‚îÄ‚îÄ Close DB session
   ‚îú‚îÄ‚îÄ Record metrics
   ‚îÇ
   ‚ñº
8. BaseResponse wrapper ‚Üí {trace_id, data, error}
```

### Code Walkthrough

```python
# main.py - Middleware stack
app.add_middleware(CORSMiddleware, allow_origins=ORIGINS)
app.add_middleware(TraceContextMiddleware)
app.add_middleware(MetricsMiddleware)

# routes/chat.py - Route handler
@router.post("/chat/stream")
@limiter.limit("30/minute")
async def stream_chat(
    request: ChatRequest,
    db: AsyncSession = Depends(get_db),
    user: User = Depends(get_current_user)
):
    # All dependencies resolved before this runs
    ctx = get_context()

    async def generate():
        async for chunk in rag_service.stream_answer(
            db=db,
            query=request.message,
            user_id=user.id,
            document_ids=request.document_ids
        ):
            yield f"data: {json.dumps({'content': chunk})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## üé≠ "Singleton Pattern ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô?"

### Problem: Creating Expensive Clients

```python
# ‚ùå Create client every request - expensive!
@router.post("/embed")
async def embed(text: str):
    client = LiteLLMClient(api_key=API_KEY)  # New client!
    return await client.embed(text)
```

### Solution: Singleton Services

```python
# services/embedding.py
_embedding_service: EmbeddingService | None = None

def get_embedding_service() -> EmbeddingService:
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService(
            model="text-embedding-004",
            api_key=settings.LITELLM_API_KEY
        )
    return _embedding_service


# Usage in route
@router.post("/embed")
async def embed(text: str):
    service = get_embedding_service()  # Reuse instance
    return await service.embed(text)
```

### Applied To

| Service | Why Singleton |
|---------|---------------|
| `EmbeddingService` | HTTP client pool reuse |
| `LLMClient` | Connection pooling |
| `VectorStore` | Configuration cached |
| `StripeClient` | API client reuse |

---

## üìä Summary: Architecture Decisions

| Decision | Alternative | Why This Choice |
|----------|-------------|-----------------|
| **Layered Architecture** | Fat controllers | Testability, reusability |
| **Async Python** | Sync | LLM calls are slow, need concurrency |
| **pgvector** | Pinecone/Weaviate | Simplicity, ACID, cost |
| **FastAPI DI** | Global state | Explicit, testable |
| **Service Singletons** | New instance per request | Performance |
| **BaseResponse wrapper** | Raw responses | Consistency, tracing |

---

*‡∏ï‡πà‡∏≠‡πÑ‡∏õ: [03-rag-deep-dive.md](./03-rag-deep-dive.md) ‚Äî Deep dive into RAG pipeline*
