# LiteLLM Proxy Configuration
# Documentation: https://docs.litellm.ai/docs/proxy/configs

model_list:
  # ============================================
  # Groq Models - Fast inference (Active)
  # ============================================
  - model_name: llama-3.3-70b
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY

  - model_name: llama-3.1-8b
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY

  - model_name: mixtral-8x7b
    litellm_params:
      model: groq/mixtral-8x7b-32768
      api_key: os.environ/GROQ_API_KEY

  # ============================================
  # Google Gemini Models (Active)
  # ============================================
  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GEMINI_API_KEY

  # ============================================
  # OpenAI Models (Uncomment when needed)
  # ============================================
  # - model_name: gpt-4o
  #   litellm_params:
  #     model: openai/gpt-4o
  #     api_key: os.environ/OPENAI_API_KEY

  # - model_name: gpt-4o-mini
  #   litellm_params:
  #     model: openai/gpt-4o-mini
  #     api_key: os.environ/OPENAI_API_KEY

  # - model_name: gpt-4-turbo
  #   litellm_params:
  #     model: openai/gpt-4-turbo
  #     api_key: os.environ/OPENAI_API_KEY

  # ============================================
  # Anthropic Models (Uncomment when needed)
  # ============================================
  # - model_name: claude-sonnet-4-20250514
  #   litellm_params:
  #     model: anthropic/claude-sonnet-4-20250514
  #     api_key: os.environ/ANTHROPIC_API_KEY

  # - model_name: claude-3-5-haiku-20241022
  #   litellm_params:
  #     model: anthropic/claude-3-5-haiku-20241022
  #     api_key: os.environ/ANTHROPIC_API_KEY

  # - model_name: claude-3-opus-20240229
  #   litellm_params:
  #     model: anthropic/claude-3-opus-20240229
  #     api_key: os.environ/ANTHROPIC_API_KEY

  # ============================================
  # Mistral Models (Uncomment when needed)
  # ============================================
  # - model_name: mistral-large-latest
  #   litellm_params:
  #     model: mistral/mistral-large-latest
  #     api_key: os.environ/MISTRAL_API_KEY

  # ============================================
  # Azure OpenAI (Uncomment when needed)
  # ============================================
  # - model_name: azure-gpt-4o
  #   litellm_params:
  #     model: azure/gpt-4o-deployment
  #     api_base: os.environ/AZURE_API_BASE
  #     api_key: os.environ/AZURE_API_KEY
  #     api_version: "2024-08-01-preview"

# Router settings for load balancing and fallbacks
router_settings:
  routing_strategy: least-busy  # Options: simple-shuffle, least-busy, usage-based-routing, latency-based-routing
  num_retries: 3
  timeout: 120
  retry_after: 5
  allowed_fails: 3

# Litellm settings
litellm_settings:
  drop_params: true  # Drop unsupported params instead of erroring
  set_verbose: false
  cache: true
  cache_params:
    type: redis  # Options: local, redis, s3
    host: redis  # If using Redis, set up a Redis container
    port: 6379
    ttl: 600

# General settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

  # Enable UI for managing models
  enable_ui: true
  ui_access_mode: all  # Options: all, admin

  # Rate limiting (optional)
  # global_max_parallel_requests: 100

  # Alerting (optional)
  # alerting:
  #   - slack
  # alerting_threshold: 300

# Environment variable configuration
environment_variables:
  LITELLM_LOG: "DEBUG"  # Options: DEBUG, INFO, WARNING, ERROR
